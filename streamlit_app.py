#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
PDFè¶‹åŠ¿å¼ºåº¦æå–å·¥å…· - Streamlit-liteç‰ˆæœ¬

åŠŸèƒ½ï¼š
1. ç›´æ¥ä»PDFæ–‡ä»¶ä¸­æå–æ–‡æœ¬å†…å®¹
2. è¯†åˆ«å¹¶æå–ã€è¶‹åŠ¿å¼ºåº¦ã€‘éƒ¨åˆ†çš„å“ç§ä¿¡æ¯
3. è¾“å‡ºæŒ‰å“ç§å’ŒæŒ‰æ—¥æœŸåˆ†ç±»çš„CSVæ–‡ä»¶
4. æ”¯æŒWebç•Œé¢æ–‡ä»¶ä¸Šä¼ 
"""

import os
import re
import csv
import io
from pathlib import Path
from datetime import datetime
from io import BytesIO
import pandas as pd
import streamlit as st

try:
    import fitz  # PyMuPDF
except ImportError:
    st.error("é”™è¯¯: ç¼ºå°‘å¿…è¦çš„ä¾èµ–åº“ PyMuPDF")
    st.error("è¯·ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å®‰è£…: pip install PyMuPDF")
    st.stop()


def extract_text_from_pdf(pdf_file, max_pages=None):
    """
    ä»PDFæ–‡ä»¶ä¸­æå–æ–‡æœ¬å†…å®¹
    
    ä½¿ç”¨PyMuPDFåº“æå–PDFæ–‡æœ¬ï¼Œæ”¯æŒæ–‡ä»¶å¯¹è±¡å’Œæ–‡ä»¶è·¯å¾„ä¸¤ç§è¾“å…¥æ–¹å¼
    
    Args:
        pdf_file: PDFæ–‡ä»¶å¯¹è±¡æˆ–æ–‡ä»¶è·¯å¾„
        max_pages: æœ€å¤§æå–é¡µæ•°ï¼ŒNoneè¡¨ç¤ºæå–æ‰€æœ‰é¡µ
        
    Returns:
        str: æå–çš„æ–‡æœ¬å†…å®¹ï¼Œå¦‚æœå¤±è´¥è¿”å›ç©ºå­—ç¬¦ä¸²
    """
    try:
        # å¦‚æœæ˜¯æ–‡ä»¶å¯¹è±¡ï¼Œè¯»å–å­—èŠ‚æ•°æ®
        if hasattr(pdf_file, 'read'):
            pdf_bytes = pdf_file.read()
            doc = fitz.open(stream=pdf_bytes, filetype="pdf")
        else:
            doc = fitz.open(pdf_file)
            
        total_pages = len(doc)
        pages_to_extract = min(total_pages, max_pages) if max_pages else total_pages
        
        st.info(f"å¼€å§‹æå–PDFæ–‡ä»¶ (å…± {total_pages} é¡µï¼Œå°†æå– {pages_to_extract} é¡µ)")
        
        # æå–æ–‡æœ¬
        text_content = []
        for page_num in range(pages_to_extract):
            page = doc.load_page(page_num)
            text = page.get_text("text")
            text_content.append(text)
            
        doc.close()
        return "\n\n".join(text_content)
    
    except Exception as e:
        st.error(f"æå–PDFæ–‡æœ¬æ—¶å‡ºé”™: {str(e)}")
        return ""


def extract_trend_strength_from_text(text, report_date=None):
    """
    ä»æ–‡æœ¬ä¸­æå–è¶‹åŠ¿å¼ºåº¦ä¿¡æ¯ï¼ˆä»…ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼ï¼‰
    
    ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…"å“ç§å è¶‹åŠ¿å¼ºåº¦: æ•°å€¼"çš„æ ¼å¼ï¼Œæå–è¶‹åŠ¿å¼ºåº¦æ•°æ®
    æ”¯æŒä¸­æ–‡ã€è‹±æ–‡ã€æ•°å­—ã€æ‹¬å·ç­‰å­—ç¬¦çš„å“ç§å
    
    Args:
        text: PDFæå–çš„æ–‡æœ¬å†…å®¹
        report_date: æŠ¥å‘Šæ—¥æœŸï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨å½“å‰æ—¥æœŸ
        
    Returns:
        list: åŒ…å«è¶‹åŠ¿å¼ºåº¦æ•°æ®çš„å­—å…¸åˆ—è¡¨ï¼Œæ¯ä¸ªå­—å…¸åŒ…å«å“ç§ã€è¶‹åŠ¿å¼ºåº¦ã€æ—¥æœŸç­‰ä¿¡æ¯
    """
    trend_data = []
    
    if not report_date:
        report_date = datetime.now().strftime("%Y-%m-%d")
    
    st.info("æ­£åœ¨æŸ¥æ‰¾è¶‹åŠ¿å¼ºåº¦ä¿¡æ¯...")
    
    # æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼ï¼šåŒ¹é…"å“ç§å è¶‹åŠ¿å¼ºåº¦: æ•°å€¼"æ ¼å¼
    # æ”¯æŒä¸­æ–‡ã€è‹±æ–‡ã€æ•°å­—ã€æ‹¬å·ç­‰å­—ç¬¦çš„å“ç§å
    patterns = [
        r'([\u4e00-\u9fa5A-Za-z0-9ï¼ˆï¼‰()]+)\s*è¶‹åŠ¿å¼ºåº¦[ï¼š:]\s*([+-]?\d+(?:\.\d+)?)',
    ]
    
    found_data = set()  # ç”¨äºå»é‡ï¼Œé¿å…é‡å¤æå–ç›¸åŒå“ç§
    extraction_details = []  # å­˜å‚¨æå–è¯¦æƒ…ç”¨äºæŠ˜å æ˜¾ç¤º
    
    for pattern in patterns:
        matches = re.findall(pattern, text)
        st.info(f"ä½¿ç”¨æ¨¡å¼ {pattern} æ‰¾åˆ° {len(matches)} ä¸ªåŒ¹é…é¡¹")
        
        for match in matches:
            if len(match) == 2:
                variety, strength = match
                variety = variety.strip()
                
                # æ•°æ®æ¸…æ´—ï¼šç§»é™¤å“ç§åä¸­çš„"è¶‹åŠ¿å¼ºåº¦"åç¼€
                if variety.endswith('è¶‹åŠ¿å¼ºåº¦'):
                    variety = variety[:-4]  # ç§»é™¤"è¶‹åŠ¿å¼ºåº¦"4ä¸ªå­—ç¬¦
                
                # æ•°æ®éªŒè¯ï¼šè¿‡æ»¤æ— æ•ˆçš„å“ç§å
                invalid_keywords = ['è¶‹åŠ¿', 'å¼ºåº¦', 'æ³¨é‡Š', 'ä¸œè', 'è¾¾å­š', 'å…¬å¸', 'æœ‰é™', 'é›†å›¢']
                if (len(variety) > 0 and 
                    not any(keyword in variety for keyword in invalid_keywords) and
                    not variety.isdigit() and
                    re.match(r'^[\u4e00-\u9fa5A-Za-z0-9ï¼ˆï¼‰()]+$', variety)):
                    try:
                        # æ•°å€¼éªŒè¯ï¼šç¡®ä¿è¶‹åŠ¿å¼ºåº¦åœ¨åˆç†èŒƒå›´å†…(-10åˆ°10)
                        strength_float = float(strength)
                        if -10 <= strength_float <= 10:
                            # è½¬æ¢ä¸ºæ•´æ•°
                            strength_int = int(round(strength_float))
                            strength_str = str(strength_int)
                            
                            # å»é‡æ£€æŸ¥ï¼šé¿å…é‡å¤æå–ç›¸åŒå“ç§
                            key = (variety, strength_str)
                            if key not in found_data:
                                found_data.add(key)
                                trend_data.append({
                                    'å“ç§': variety,
                                    'è¶‹åŠ¿å¼ºåº¦': strength_str,  # ä¿å­˜æ•´æ•°å­—ç¬¦ä¸²
                                    'æ—¥æœŸ': report_date
                                })
                                extraction_details.append(f"æå–: {variety} = {strength_str}")
                    except ValueError:
                        extraction_details.append(f"è·³è¿‡æ— æ•ˆæ•°å€¼: {variety} = {strength}")
                        continue
    
    # æŠ˜å æ˜¾ç¤ºæå–è¯¦æƒ…
    if extraction_details:
        with st.expander(f"ğŸ“‹ æå–è¯¦æƒ… ({len(extraction_details)} æ¡)", expanded=False):
            for detail in extraction_details:
                if "æå–:" in detail:
                    st.success(detail)
                else:
                    st.warning(detail)
    
    if trend_data:
        st.success(f"å…±æå–åˆ° {len(trend_data)} ä¸ªå“ç§çš„è¶‹åŠ¿å¼ºåº¦ä¿¡æ¯")
    else:
        st.error("æœªæ‰¾åˆ°ä»»ä½•è¶‹åŠ¿å¼ºåº¦ä¿¡æ¯")
        # æ˜¾ç¤ºè°ƒè¯•ä¿¡æ¯
        lines_with_trend = [line.strip() for line in text.split('\n') 
                           if 'è¶‹åŠ¿å¼ºåº¦' in line and line.strip()]
        if lines_with_trend:
            st.info("åŒ…å«'è¶‹åŠ¿å¼ºåº¦'çš„æ–‡æœ¬è¡Œ:")
            for line in lines_with_trend[:5]:  # åªæ˜¾ç¤ºå‰5è¡Œ
                st.text(f"  {line}")
        else:
            st.warning("æ–‡æ¡£ä¸­æœªæ‰¾åˆ°åŒ…å«'è¶‹åŠ¿å¼ºåº¦'çš„æ–‡æœ¬è¡Œ")
    
    return trend_data


def save_trend_strength_pivot_csv(all_trend_data, output_dir=None, incremental=True):
    """
    å°†è¶‹åŠ¿å¼ºåº¦æ•°æ®ä¿å­˜ä¸ºé€è§†è¡¨æ ¼å¼ï¼Œå¹¶æ ‡è®°å‘ç”Ÿå˜åŒ–çš„å“ç§
    
    æ ¸å¿ƒåŠŸèƒ½ï¼š
    1. åˆå¹¶å†å²æ•°æ®å’Œæ–°æ•°æ®ï¼ˆå¢é‡æ›´æ–°ï¼‰
    2. åˆ›å»ºé€è§†è¡¨ï¼ˆæ—¥æœŸä¸ºè¡Œï¼Œå“ç§ä¸ºåˆ—ï¼‰
    3. æŒ‰æœ€æ–°æ—¥æœŸçš„è¶‹åŠ¿å€¼æ’åºåˆ—ï¼ˆæœ‰å€¼åœ¨å‰ï¼Œæ— å€¼åœ¨åï¼‰
    4. ä¿å­˜æ•°æ®åˆ°CSVæ–‡ä»¶å®ç°æŒä¹…åŒ–
    
    Args:
        all_trend_data: å½“å‰æå–çš„è¶‹åŠ¿å¼ºåº¦æ•°æ®åˆ—è¡¨
        output_dir: è¾“å‡ºç›®å½•ï¼ˆåœ¨Streamlitä¸­ä¸ä½¿ç”¨ï¼‰
        incremental: æ˜¯å¦å¯ç”¨å¢é‡æ›´æ–°æ¨¡å¼ï¼Œé»˜è®¤ä¸ºTrue
    
    Returns:
        DataFrame: é€è§†è¡¨ï¼ˆæ—¥æœŸä¸ºè¡Œï¼Œå“ç§ä¸ºåˆ—ï¼‰
    """
    if not all_trend_data:
        return None
    
    # åˆ›å»ºæ–°æ•°æ®çš„DataFrame
    new_df = pd.DataFrame(all_trend_data)
    
    # å¦‚æœå¯ç”¨å¢é‡æ›´æ–°ä¸”å­˜åœ¨å†å²æ•°æ®ï¼Œåˆ™åˆå¹¶
    if incremental and 'historical_data' in st.session_state and st.session_state.historical_data:
        # é™é»˜å¢é‡æ›´æ–°ï¼Œé¿å…é¡µé¢é¡¶éƒ¨å‡ºç°æç¤º
        historical_df = pd.DataFrame(st.session_state.historical_data)
        
        # åˆå¹¶å†å²æ•°æ®å’Œæ–°æ•°æ®
        combined_df = pd.concat([historical_df, new_df], ignore_index=True)
        
        # å»é‡ï¼šåŒä¸€æ—¥æœŸåŒä¸€å“ç§åªä¿ç•™æœ€æ–°çš„è®°å½•
        combined_df = combined_df.drop_duplicates(subset=['æ—¥æœŸ', 'å“ç§'], keep='last')
        
        # åˆå¹¶åæ•°æ®é‡æ—¥å¿—çœç•¥
    else:
        combined_df = new_df
        # ä½¿ç”¨æ–°æ•°æ®æ—¥å¿—çœç•¥
    
    # æ›´æ–°session_stateä¸­çš„å†å²æ•°æ®
    st.session_state.historical_data = combined_df.to_dict('records')
    
    # ä¿å­˜åˆ°CSVæ–‡ä»¶ä»¥å®ç°æŒä¹…åŒ–
    try:
        data_dir = Path("./data")
        data_dir.mkdir(exist_ok=True)
        csv_file_path = data_dir / "trend_strength_data.csv"
        combined_df.to_csv(csv_file_path, index=False, encoding='utf-8-sig')
        # ä¿å­˜CSVçš„æç¤ºçœç•¥
    except Exception as e:
        st.error(f"ä¿å­˜CSVæ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
    
    # åˆ›å»ºé€è§†è¡¨ - ç»Ÿä¸€ä½¿ç”¨å­—ç¬¦ä¸²å­—æ®µï¼Œç¡®ä¿æ˜¾ç¤ºä¸€è‡´æ€§
    # ç¡®ä¿åªä½¿ç”¨å¿…è¦çš„å­—æ®µï¼Œæ’é™¤å¯èƒ½å­˜åœ¨çš„è¶‹åŠ¿å¼ºåº¦_floatå­—æ®µ
    if 'è¶‹åŠ¿å¼ºåº¦_float' in combined_df.columns:
        # å¦‚æœå­˜åœ¨è¶‹åŠ¿å¼ºåº¦_floatå­—æ®µï¼Œå…ˆåˆ é™¤å®ƒ
        combined_df = combined_df.drop(columns=['è¶‹åŠ¿å¼ºåº¦_float'])
    
    # ç¡®ä¿è¶‹åŠ¿å¼ºåº¦å­—æ®µéƒ½æ˜¯æ•´æ•°æ ¼å¼çš„å­—ç¬¦ä¸²
    if 'è¶‹åŠ¿å¼ºåº¦' in combined_df.columns:
        # å°†è¶‹åŠ¿å¼ºåº¦å­—æ®µè½¬æ¢ä¸ºæ•´æ•°æ ¼å¼
        combined_df['è¶‹åŠ¿å¼ºåº¦'] = combined_df['è¶‹åŠ¿å¼ºåº¦'].astype(str).apply(
            lambda x: str(int(float(x))) if x.replace('.', '').replace('-', '').isdigit() else x
        )
    
    pivot_df = combined_df.pivot_table(
        index='æ—¥æœŸ', 
        columns='å“ç§', 
        values='è¶‹åŠ¿å¼ºåº¦', 
        aggfunc='first',  # ä½¿ç”¨ç¬¬ä¸€ä¸ªå€¼ï¼Œå› ä¸ºå­—ç¬¦ä¸²ä¸èƒ½æ±‚å¹³å‡
        fill_value='--'  # ä½¿ç”¨--è¡¨ç¤ºæ— æ•°æ®ï¼Œä¾¿äºè¾¨è¯†
    )
    
    # ç¡®ä¿æ‰€æœ‰å€¼éƒ½æ˜¯å­—ç¬¦ä¸²ç±»å‹ï¼Œé¿å…Arrowè½¬æ¢é”™è¯¯
    pivot_df = pivot_df.astype(str)
    # æŒ‰æ—¥æœŸæ’åºï¼ˆä»æ–°åˆ°æ—§ï¼‰
    pivot_df = pivot_df.sort_index(ascending=False)
    
    # åˆ—æ’åºä¼˜åŒ–ï¼šæŒ‰æœ€æ–°æ—¥æœŸåˆ†ç»„æ’åº
    # ä»å·¦åˆ°å³é¡ºåºï¼šè¶‹åŠ¿>0ï¼Œè¶‹åŠ¿<0ï¼Œè¶‹åŠ¿=0ï¼Œæœªæå–åˆ°ï¼ˆ--ï¼‰
    if not pivot_df.empty:
        latest_row = pivot_df.iloc[0]

        def classify_value(v: str) -> str:
            try:
                n = float(v)
            except Exception:
                return 'missing'
            if n > 0:
                return 'pos'
            if n < 0:
                return 'neg'
            return 'zero'

        groups = {'pos': [], 'neg': [], 'zero': [], 'missing': []}
        for col in pivot_df.columns:
            grp = classify_value(latest_row.get(col, '--'))
            groups[grp].append(col)

        ordered_cols = groups['pos'] + groups['neg'] + groups['zero'] + groups['missing']
        # ä»…åœ¨åˆ—é›†ä¸€è‡´æ—¶é‡æ’ï¼Œé¿å…æ½œåœ¨ç¼ºå¤±
        if set(ordered_cols) == set(pivot_df.columns):
            pivot_df = pivot_df[ordered_cols]
    
    # ç»´åº¦ä¿¡æ¯æç¤ºçœç•¥
    
    return pivot_df


# æ ·å¼è¾…åŠ©ï¼šä»…ä¸ºâ€œæœ€æ–°æ—¥æœŸâ€ä¸€è¡Œæ·»åŠ é¢œè‰²ï¼ˆæ­£æ•°çº¢ï¼Œè´Ÿæ•°ç»¿ï¼‰
def style_latest_date_row(df: pd.DataFrame) -> pd.DataFrame:
    """è¿”å›ä¸dfåŒå½¢çŠ¶çš„æ ·å¼DataFrameï¼Œä»…ä¸ºç¬¬ä¸€è¡Œï¼ˆæœ€æ–°æ—¥æœŸï¼‰ç€è‰²ã€‚
    - æ­£æ•°ï¼šçº¢è‰²
    - è´Ÿæ•°ï¼šç»¿è‰²
    å…¶ä½™å•å…ƒæ ¼ä¸ç€è‰²ã€‚
    æ”¯æŒå•å…ƒæ ¼ä¸­å¸¦æœ‰â†‘/â†“ç®­å¤´æˆ–éæ•°å­—å ä½ç¬¦ï¼ˆå¦‚--ï¼‰ã€‚
    """
    styles = pd.DataFrame('', index=df.index, columns=df.columns)
    if df.empty:
        return styles

    latest_idx = df.index[0]

    def parse_number(val: str):
        if val is None:
            return None
        try:
            s = str(val)
            # å»æ‰ç®­å¤´å’Œç©ºæ ¼
            s = s.replace('â†‘', '').replace('â†“', '').strip()
            if s in ('', '--'):
                return None
            return float(s)
        except Exception:
            return None

    for col in df.columns:
        num = parse_number(df.loc[latest_idx, col])
        if num is None:
            continue
        if num > 0:
            styles.loc[latest_idx, col] = 'color: red'
        elif num < 0:
            styles.loc[latest_idx, col] = 'color: green'
        # ç­‰äº0ä¸ç€è‰²

    return styles



def analyze_pdf_trend_strength(pdf_file, filename=None):
    """
    åˆ†æPDFæ–‡ä»¶ä¸­çš„è¶‹åŠ¿å¼ºåº¦ä¿¡æ¯
    
    Args:
        pdf_file: PDFæ–‡ä»¶å¯¹è±¡
        filename: æ–‡ä»¶å
        
    Returns:
        æå–çš„è¶‹åŠ¿å¼ºåº¦æ•°æ®å’Œç»Ÿè®¡ä¿¡æ¯
    """
    st.info(f"å¼€å§‹åˆ†æPDFæ–‡ä»¶: {filename or 'uploaded file'}")
    
    # ä»PDFæå–æ–‡æœ¬
    text_content = extract_text_from_pdf(pdf_file)
    if not text_content:
        st.error("æ— æ³•ä»PDFæå–æ–‡æœ¬å†…å®¹")
        return [], {}
    
    st.info(f"æå–çš„æ–‡æœ¬é•¿åº¦: {len(text_content)} å­—ç¬¦")
    
    # åªä»æ–‡ä»¶åæå–æ—¥æœŸ
    report_date = None
    if filename:
        st.info(f"æ­£åœ¨ä»æ–‡ä»¶åæå–æ—¥æœŸ: {filename}")
        # ä¼˜å…ˆåŒ¹é…8ä½è¿ç»­æ•°å­—ï¼ˆYYYYMMDDæ ¼å¼ï¼‰
        date_match = re.search(r'(\d{8})', filename)
        if date_match:
            date_str = date_match.group(1)
            try:
                # éªŒè¯æ—¥æœŸæ ¼å¼å¹¶è½¬æ¢
                year, month, day = date_str[:4], date_str[4:6], date_str[6:8]
                report_date = f"{year}-{month}-{day}"
                # éªŒè¯æ—¥æœŸæœ‰æ•ˆæ€§
                datetime.strptime(report_date, '%Y-%m-%d')
                st.success(f"ä»æ–‡ä»¶åæå–åˆ°æ—¥æœŸ: {report_date}")
            except ValueError:
                st.warning(f"æ–‡ä»¶åä¸­çš„æ—¥æœŸæ ¼å¼æ— æ•ˆ: {date_str}")
                report_date = None
        else:
            st.warning("æ–‡ä»¶åä¸­æœªæ‰¾åˆ°8ä½æ•°å­—æ—¥æœŸæ ¼å¼ï¼ˆYYYYMMDDï¼‰")
    
    # å¦‚æœæ–‡ä»¶åä¸­æ²¡æœ‰æ‰¾åˆ°æ—¥æœŸï¼Œä½¿ç”¨å½“å‰æ—¥æœŸ
    if not report_date:
        report_date = datetime.now().strftime("%Y-%m-%d")
        st.warning(f"æœªæ‰¾åˆ°æœ‰æ•ˆæ—¥æœŸï¼Œä½¿ç”¨å½“å‰æ—¥æœŸ: {report_date}")
    
    # æå–è¶‹åŠ¿å¼ºåº¦ä¿¡æ¯
    trend_data = extract_trend_strength_from_text(text_content, report_date)
    
    # ä¸ºæ¯æ¡æ•°æ®æ·»åŠ æ–‡ä»¶åä¿¡æ¯
    if trend_data and filename:
        for item in trend_data:
            item['æ–‡ä»¶å'] = filename
    
    # ç»Ÿè®¡ä¿¡æ¯
    stats = {}
    if trend_data:
        for item in trend_data:
            category = item.get('ç±»åˆ«', 'æœªåˆ†ç±»')
            stats[category] = stats.get(category, 0) + 1
    
    return trend_data, stats


def main():
    """
    Streamlitä¸»åº”ç”¨
    """
    st.set_page_config(
        page_title="PDFè¶‹åŠ¿å¼ºåº¦æå–å·¥å…·",
        page_icon="ğŸ“Š",
        layout="wide"
    )
    
    # å–æ¶ˆé¡µé¢ä¸»æ ‡é¢˜å±•ç¤º
    
    # åˆ›å»ºæ•°æ®ç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    data_dir = Path("./data")
    data_dir.mkdir(exist_ok=True)
    
    # CSVæ–‡ä»¶è·¯å¾„
    csv_file_path = data_dir / "trend_strength_data.csv"
    
    # åˆå§‹
    # åŒ–session state
    if 'historical_data' not in st.session_state:
        # å°è¯•ä»CSVæ–‡ä»¶åŠ è½½å†å²æ•°æ®
        if csv_file_path.exists():
            try:
                df = pd.read_csv(csv_file_path, encoding='utf-8-sig')
                
                # æ¸…ç†å†å²æ•°æ®ï¼Œç§»é™¤è¶‹åŠ¿å¼ºåº¦_floatå­—æ®µ
                if 'è¶‹åŠ¿å¼ºåº¦_float' in df.columns:
                    df = df.drop(columns=['è¶‹åŠ¿å¼ºåº¦_float'])
                    st.info("å·²æ¸…ç†å†å²æ•°æ®ä¸­çš„è¶‹åŠ¿å¼ºåº¦_floatå­—æ®µ")
                
                st.session_state.historical_data = df.to_dict('records')
                # å–æ¶ˆCSVåŠ è½½æˆåŠŸæç¤º
            except Exception as e:
                st.error(f"åŠ è½½CSVæ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
                st.session_state.historical_data = []
        else:
            st.session_state.historical_data = []
    
    # å°†æ–‡ä»¶ä¸Šä¼ åŠŸèƒ½æ”¾åœ¨æœ€ä¸Šæ–¹
    uploaded_files = st.file_uploader(
        "é€‰æ‹©PDFæ–‡ä»¶",
        type=['pdf'],
        accept_multiple_files=True,
        help="è¯·ä¸Šä¼ åŒ…å«è¶‹åŠ¿å¼ºåº¦ä¿¡æ¯çš„PDFæ–‡ä»¶ï¼Œæ”¯æŒæ‰¹é‡ä¸Šä¼ "
    )
    
    if uploaded_files:
        if len(uploaded_files) == 1:
            st.success(f"å·²ä¸Šä¼ æ–‡ä»¶: {uploaded_files[0].name}")
        else:
            st.success(f"å·²ä¸Šä¼  {len(uploaded_files)} ä¸ªæ–‡ä»¶")
            with st.expander("æŸ¥çœ‹ä¸Šä¼ çš„æ–‡ä»¶åˆ—è¡¨", expanded=False):
                for i, file in enumerate(uploaded_files, 1):
                    st.write(f"{i}. {file.name}")
        
        # åˆ†ææŒ‰é’®
        if st.button("å¼€å§‹åˆ†æ", type="primary"):
            with st.spinner(f"æ­£åœ¨åˆ†æ {len(uploaded_files)} ä¸ªPDFæ–‡ä»¶ï¼Œè¯·ç¨å€™..."):
                all_trend_data = []
                all_stats = {}
                
                # å¤„ç†æ¯ä¸ªä¸Šä¼ çš„æ–‡ä»¶
                for i, uploaded_file in enumerate(uploaded_files, 1):
                    # åˆ†æPDF
                    trend_data, stats = analyze_pdf_trend_strength(uploaded_file, uploaded_file.name)
                    
                    if trend_data:
                        all_trend_data.extend(trend_data)
                        
                        # åˆå¹¶ç»Ÿè®¡ä¿¡æ¯
                        for category, count in stats.items():
                            all_stats[category] = all_stats.get(category, 0) + count
                    else:
                        st.warning(f"âš ï¸ {uploaded_file.name}: æœªèƒ½æå–åˆ°æ•°æ®")
                
                # æ˜¾ç¤ºæ±‡æ€»ç»“æœ
                if all_trend_data:
                    st.success(f"âœ… åˆ†æå®Œæˆï¼š{len(all_trend_data)} æ¡è¶‹åŠ¿å¼ºåº¦ä¿¡æ¯")
                    
                    # ä½¿ç”¨åˆå¹¶åçš„æ•°æ®è¿›è¡Œåç»­å¤„ç†
                    trend_data = all_trend_data
                    stats = all_stats
                else:
                    st.error("æœªèƒ½æå–åˆ°è¶‹åŠ¿å¼ºåº¦ä¿¡æ¯")
                    st.stop()
                
                if trend_data:
                    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
                    col1, col2, col3 = st.columns(3)
                    with col1:
                        st.metric("æ€»å“ç§æ•°", len(trend_data))
                    with col2:
                        st.metric("æˆåŠŸæ–‡ä»¶æ•°", len(uploaded_files))
                    with col3:
                        st.metric("æœ€æ–°æ—¥æœŸ", trend_data[0]['æ—¥æœŸ'] if trend_data else "æ— ")
                    
                    # æ˜¾ç¤ºæå–ç»“æœ
                    st.subheader("ğŸ“‹ æå–ç»“æœ")
                    df = pd.DataFrame(trend_data)
                    st.dataframe(df, width='stretch')
                    
                    # ç”Ÿæˆé€è§†è¡¨ï¼ˆå¯ç”¨å¢é‡æ›´æ–°ï¼‰
                    st.subheader("ğŸ“Š é€è§†è¡¨åˆ†æ")
                    pivot_df = save_trend_strength_pivot_csv(trend_data, incremental=True)
                    
                    if pivot_df is not None:
                        st.write("**è¶‹åŠ¿å¼ºåº¦é€è§†è¡¨:**")
                        # ä¸ºæœ€æ–°æ—¥æœŸä¸€è¡ŒåŠ è‰²ï¼šæ­£æ•°çº¢ã€è´Ÿæ•°ç»¿
                        styled = pivot_df.astype(str).style.apply(style_latest_date_row, axis=None)
                        st.dataframe(styled, width='stretch')
                    
                    
                else:
                    st.warning("æœªæ‰¾åˆ°è¶‹åŠ¿å¼ºåº¦ä¿¡æ¯")
                    st.info("è¯·ç¡®ä¿PDFæ–‡ä»¶åŒ…å«ã€è¶‹åŠ¿å¼ºåº¦ã€‘éƒ¨åˆ†çš„å†…å®¹")

    # æ˜¾ç¤ºå†å²æ•°æ®ç»Ÿè®¡
    if st.session_state.historical_data:
        historical_df = pd.DataFrame(st.session_state.historical_data)
        unique_dates = historical_df['æ—¥æœŸ'].nunique()
        unique_varieties = historical_df['å“ç§'].nunique()
        
        # è·å–æœ€æ–°æ—¥æœŸ
        latest_date = historical_df['æ—¥æœŸ'].max()
        # å–æ¶ˆé¡µé¢ä¸Šæ–¹çš„ç»Ÿè®¡æç¤º
        
        # ç”Ÿæˆé€è§†è¡¨ï¼ˆä½¿ç”¨å†å²æ•°æ®ï¼‰
        # st.subheader("ğŸ“Š å†å²æ•°æ®åˆ†æ")
        pivot_df = save_trend_strength_pivot_csv(st.session_state.historical_data, incremental=False)
        
        if pivot_df is not None:
            # æ˜¯å¦æ˜¾ç¤ºå®Œæ•´å†å²æ•°æ®
            show_full = st.checkbox("æ˜¾ç¤ºå®Œæ•´å†å²æ•°æ®è¡¨æ ¼", value=False)
            if not show_full:
                pivot_df = pivot_df.head(10)
            title_txt = "å†å²æ•°æ®é€è§†è¡¨ï¼ˆå®Œæ•´ï¼‰" if show_full else "å†å²æ•°æ®é€è§†è¡¨ï¼ˆæœ€æ–°10ä¸ªæ—¥æœŸï¼‰"
            st.write(f"**{title_txt}:**")
            # æ„é€ å˜åŒ–æ ‡è®°ï¼šä¸ä¸Šä¸€æ—¥æ¯”è¾ƒï¼Œå˜åŠ¨ç”¨ç®­å¤´æ ‡è®°
            pivot_str = pivot_df.astype(str)
            # æ•°å€¼åŒ–ç”¨äºæ¯”è¾ƒï¼Œ'--' è½¬ä¸ºç©º
            pivot_num = pivot_str.replace('--', pd.NA).apply(pd.to_numeric, errors='coerce')
            prev_num = pivot_num.shift(-1)  # ä¸Šä¸€æ—¥ï¼ˆå› æ—¥æœŸä»æ–°åˆ°æ—§æ’åºï¼‰

            up_mask = pivot_num.notna() & prev_num.notna() & (pivot_num > prev_num)
            down_mask = pivot_num.notna() & prev_num.notna() & (pivot_num < prev_num)
            marker = pd.DataFrame('', index=pivot_str.index, columns=pivot_str.columns)
            # ä»…æ ‡æ³¨ä¸Šå‡/ä¸‹é™ï¼Œä¸å¯¹ç¼ºå¤±å˜åŒ–æ ‡è®°
            marker = marker.mask(up_mask, 'â†‘').mask(down_mask, 'â†“')

            pivot_with_marks = pivot_str + marker
            # ä¸ºæœ€æ–°æ—¥æœŸä¸€è¡ŒåŠ è‰²ï¼šæ­£æ•°çº¢ã€è´Ÿæ•°ç»¿
            styled = (pivot_with_marks
                      .style
                      .apply(style_latest_date_row, axis=None))
            st.dataframe(styled, width='stretch')

        # åˆ é™¤æŒ‡å®šæ—¥æœŸçš„æ•°æ®
        with st.expander("åˆ é™¤æŒ‡å®šæ—¥æœŸçš„æ•°æ®", expanded=False):
            try:
                available_dates = sorted(historical_df['æ—¥æœŸ'].unique().tolist(), reverse=True)
            except Exception:
                available_dates = []
            if available_dates:
                selected_date = st.selectbox("é€‰æ‹©è¦åˆ é™¤çš„æ—¥æœŸ", options=available_dates, index=0)
                if st.button("ğŸ—‘ï¸ åˆ é™¤æ‰€é€‰æ—¥æœŸæ•°æ®", help="å°†ä»å†å²è®°å½•ä¸CSVä¸­ç§»é™¤è¯¥æ—¥æœŸçš„æ‰€æœ‰æ•°æ®"):
                    remaining = [item for item in st.session_state.historical_data if item.get('æ—¥æœŸ') != selected_date]
                    removed_count = len(st.session_state.historical_data) - len(remaining)
                    st.session_state.historical_data = remaining
                    # åŒæ­¥æ›´æ–°CSVæ–‡ä»¶
                    try:
                        if remaining:
                            pd.DataFrame(remaining).to_csv(csv_file_path, index=False, encoding='utf-8-sig')
                        else:
                            if csv_file_path.exists():
                                csv_file_path.unlink()
                        st.success(f"å·²åˆ é™¤ {selected_date} çš„ {removed_count} æ¡è®°å½•")
                    except Exception as e:
                        st.error(f"æ›´æ–°CSVæ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
                    st.rerun()
            else:
                st.info("å½“å‰æ— å¯åˆ é™¤çš„æ—¥æœŸ")
        
        # æŒ‰éœ€ç§»é™¤æ¸…é™¤å†å²æ•°æ®åŠŸèƒ½ï¼ˆå·²å–æ¶ˆï¼‰
    
    
    


if __name__ == "__main__":
    main()
